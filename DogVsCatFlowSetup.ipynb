{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dog Vs Cat: A CNN based Dog/Cat Classifer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enviroment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, ELU\n",
    "from keras.layers.convolutional import Cropping2D, Convolution2D, MaxPooling2D\n",
    "from keras.layers.core import Lambda, Dense, Activation, Flatten\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "\n",
    "TRAIN_DATA_PATH_LIST = [\"./train\"]\n",
    "TEST_DATA_PATH_LIST = [\"./test\"]\n",
    "\n",
    "LABEL_LIST = [\"dog\", \"cat\"]\n",
    "\n",
    "DO_VISUALIZE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(DATA_PATH, LABEL_LIST):\n",
    "    labels = LABEL_LIST\n",
    "    labeled_data = {}\n",
    "    for dir in DATA_PATH: \n",
    "        print(\"processing data from: \" + dir)\n",
    "        for key in labels:           \n",
    "            labeled_data[key] = [file for file in glob.glob(os.path.join(dir, key + '*.jpg'))]\n",
    "            print(\"total \" + str(len(labeled_data[key])) + \" \" + key + \" images found.\")\n",
    "        \n",
    "    return labeled_data\n",
    "\n",
    "\n",
    "def raw_data_analysis(data):\n",
    "    image_size_counter = {}    \n",
    "    dimension_shape = [\"height\", \"width\"]\n",
    "    for key in data.keys():\n",
    "        print(\"processing: \" + key)\n",
    "        image_size_counter[key] = {}\n",
    "        for i in range(len(dimension_shape)):\n",
    "            print(dimension_shape[i])\n",
    "            image_size_counter[key][dimension_shape[i]] = Counter()            \n",
    "            for img_path in data[key]:\n",
    "                img = cv2.imread(img_path)            \n",
    "                image_size_counter[key][dimension_shape[i]][img.shape[i]] += 1\n",
    "                #image_size_counter[key][dimension_shape[0]][img.shape[1]] += 1\n",
    "        \n",
    "            print(\"most common \" + key + \" image \" + dimension_shape[i] + \": \" , image_size_counter[key][dimension_shape[i]].most_common(10))\n",
    "        \n",
    "   \n",
    "    return image_size_counter\n",
    "        \n",
    "    \n",
    "\n",
    "data = load_data(TRAIN_DATA_PATH_LIST, LABEL_LIST)\n",
    "\n",
    "\n",
    "temp_counter_dict = raw_data_analysis(data)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_label_list = list(temp_counter_dict.keys())\n",
    "#dimension_list = list(temp_counter_dict[data_label_list[0]].keys())\n",
    "\n",
    "        \n",
    "def visualize_image_size_distribution(counter_dict):\n",
    "    data_label_list = list(counter_dict.keys())\n",
    "    dimension_list =   list(counter_dict[data_label_list[0]].keys())   \n",
    "    for dimension in dimension_list:    \n",
    "        f, ax = plt.subplots(1, 2)\n",
    "        for i in range(len(data_label_list)):\n",
    "            x = sorted(counter_dict[data_label_list[i]][dimension].keys())\n",
    "            y = [counter_dict[data_label_list[i]][dimension][j] for j in x]\n",
    "            ax[i].bar(x, y, width = 4, align='center', color= \"green\", alpha=0.4)            \n",
    "            ax[i].set_title(data_label_list[i] + \" image \" + dimension + \" distribution\")\n",
    "            ax[i].set_xlabel(\"size (in pixel)\")\n",
    "            ax[i].set_ylabel(\"count\")\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "visualize_image_size_distribution(temp_counter_dict)\n",
    "\n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# resize all training image to the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def resize_image(image, shape = (500, 500)):\n",
    "    resized_image = cv2.resize(image, shape)\n",
    "    return resized_image\n",
    "\n",
    "img = cv2.imread(data[\"cat\"][42])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.subplot(121).imshow(img)\n",
    "r = resize_image(img, (500, 500))\n",
    "plt.subplot(122).imshow(r)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def one_hot_encoder(label_list):\n",
    "    encoder = LabelEncoder()\n",
    "    test = encoder.fit_transform(label_list)\n",
    "    #print(test)\n",
    "    #print(encoder.inverse_transform(test))\n",
    "    return encoder\n",
    "\n",
    "def sample_generator(samples, batch_size=10):\n",
    "    sample_list = []\n",
    "    label_list = list(samples.keys())\n",
    "    print(label_list)\n",
    "    \n",
    "    encoder = one_hot_encoder(label_list)\n",
    "    \n",
    "    for label in label_list:\n",
    "        for image_path in samples[label]:\n",
    "            sample_list.append([label, image_path])    \n",
    "    \n",
    "    sample_list = shuffle(sample_list)\n",
    "    num_samples = len(sample_list)\n",
    "    print(\"num of samples: \", num_samples)\n",
    "    \n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        sample_list = shuffle(sample_list)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = sample_list[offset:offset + batch_size]\n",
    "            images = []\n",
    "            one_hot_labels = []\n",
    "            \n",
    "            for sample in batch_samples:\n",
    "                image = cv2.imread(sample[1])\n",
    "                image = resize_image(image)\n",
    "                one_hot_label = encoder.transform([sample[0]])\n",
    "                \n",
    "                #plt.imshow(image)\n",
    "                #plt.show()\n",
    "                #print(sample[0] +  \" one hot label: \" , one_hot_label)\n",
    "                \n",
    "                images.append(image)\n",
    "                one_hot_labels.append(one_hot_label)\n",
    "            \n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(one_hot_labels)\n",
    "            y_train = np_utils.to_categorical(y_train)\n",
    "            #print(\"y_train\", y_train)\n",
    "            yield shuffle(X_train, y_train)\n",
    "    \n",
    " \n",
    "\n",
    "    \n",
    "for i in range(5):\n",
    "    next(sample_generator(data))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_setup(input_shape = (500, 500, 3), num_classes = 2):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32,3,3, dim_ordering='tf', input_shape = input_shape))    \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Convolution2D(32,3,3))    \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Convolution2D(64,3,3))    \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    model.compile(loss = 'categorical_crossentropy',  optimizer=\"adam\", metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "       \n",
    "\n",
    "model_setup()    \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_dict_to_list(sample_dict):\n",
    "    sample_list = []\n",
    "    label_list = list(sample_dict.keys())\n",
    "    print(label_list)  \n",
    "   \n",
    "    for label in label_list:\n",
    "        for image_path in sample_dict[label]:\n",
    "            sample_list.append([label, image_path])    \n",
    "\n",
    "    num_samples = len(sample_list)\n",
    "    print(\"num of samples: \", num_samples)\n",
    "    \n",
    "    #print(sample_list)\n",
    "    \n",
    "    return sample_list\n",
    "\n",
    "sample_dict_to_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_one_hot_encoding(labe_list):\n",
    "    encoder = LabelEncoder()\n",
    "    transfomed_label = encoder.fit_transform(labe_list)\n",
    "    one_hot_encoded_labels =  np_utils.to_categorical(transfomed_label)\n",
    "    one_hot_label_dict = {}\n",
    "    for i  in range(len(labe_list)):\n",
    "        one_hot_label_dict[labe_list[i]] = one_hot_encoded_labels[i] \n",
    "    \n",
    "    print(\"one hot encoding: \", one_hot_label_dict)\n",
    "    return one_hot_label_dict\n",
    "    \n",
    "    \n",
    "one_hot_encoding_dict = label_one_hot_encoding(LABEL_LIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_samples(DATA_PATH, one_hot_encoding_dict):\n",
    "    labels = one_hot_encoding_dict.keys()    \n",
    "    samples = [] # [one_hot_label, image_path]\n",
    "    temp_size = 0\n",
    "    for dir in DATA_PATH: \n",
    "        print(\"loading data from: \" + dir)\n",
    "        for key in labels:           \n",
    "            samples += [[one_hot_encoding_dict[key],file] for file in glob.glob(os.path.join(dir, key + '*.jpg'))]\n",
    "            temp_size = len(samples) - temp_size    \n",
    "            print(\"total \" + str(temp_size) + \" \" + key + \" images loaded.\")\n",
    "            temp_size = len(samples)\n",
    "        \n",
    "        print(\"total \" + str(temp_size) + \" raw data samples loaded.\")\n",
    "        \n",
    "    return samples    \n",
    "\n",
    "raw_samples = load_samples(TRAIN_DATA_PATH_LIST, one_hot_encoding_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def raw_data_analysis(raw_sample_list, one_hot_encoding_dict):\n",
    "    \n",
    "    inverse_one_hot_encoding_dict = {tuple(v): k for k, v in one_hot_encoding_dict.items()}\n",
    "    \n",
    "    print(inverse_one_hot_encoding_dict)\n",
    "    \n",
    "    image_size_counter = {}   \n",
    "    dimension_shape = [\"height\", \"width\"]\n",
    "    \n",
    "    for sample in raw_sample_list:   \n",
    "        label = inverse_one_hot_encoding_dict[tuple(sample[0])]\n",
    "        if label  not in image_size_counter:\n",
    "            image_size_counter[label]= {}\n",
    "            for i in range(len(dimension_shape)):\n",
    "                image_size_counter[label][dimension_shape[i]] = Counter() \n",
    "        \n",
    "        img = cv2.imread(sample[1])\n",
    "        for i in range(len(dimension_shape)):\n",
    "            image_size_counter[label][dimension_shape[i]][img.shape[i]] += 1\n",
    "\n",
    "            \n",
    "    for label in image_size_counter.keys():\n",
    "        for dim in dimension_shape:\n",
    "             print(\"most common \" + label + \" image \" + dim + \": \" , image_size_counter[label][dim].most_common(10))\n",
    "            \n",
    "\n",
    "    return image_size_counter\n",
    "\n",
    "print(raw_samples[42])\n",
    "\n",
    "image_shape_counter = raw_data_analysis(raw_samples, one_hot_encoding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_image_size_distribution(image_shape_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_generator(samples, batch_size=10):\n",
    "    \n",
    "    while True:\n",
    "        samples = shuffle(samples)\n",
    "        num_samples = len(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset + batch_size]\n",
    "            images = []\n",
    "            one_hot_labels = []\n",
    "            \n",
    "            for sample in batch_samples:\n",
    "                image = cv2.imread(sample[1])\n",
    "                image = resize_image(image)\n",
    "\n",
    "                images.append(image)\n",
    "                one_hot_labels.append(sample[0])\n",
    "                #plt.imshow(image)\n",
    "                #plt.show()\n",
    "            \n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(one_hot_labels)\n",
    "            \n",
    "            #print(\"y_train\", y_train)\n",
    "            yield shuffle(X_train, y_train)\n",
    "    \n",
    " \n",
    "\n",
    "    \n",
    "for i in range(3):\n",
    "    next(sample_generator(raw_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flow_setup():\n",
    "    train_validation_samples, test_samples = train_test_split(raw_samples, test_size = 0.2, random_state = 42)\n",
    "    train_samples, validation_samples = train_test_split(train_validation_samples, test_size = 0.2, random_state = 42)\n",
    "    print(\"train sample count: \", len(train_samples), \"\\nvalidation sample count: \", len(validation_samples), \"\\ntest sample count: \", len(test_samples))\n",
    "    print(\"sample data example:\\n\", train_samples[random.randint(0, len(train_samples))])\n",
    "    \n",
    "    train_generator = sample_generator(train_samples, batch_size = 32)\n",
    "    validation_generator = sample_generator(validation_samples, batch_size = 32)\n",
    "    test_generator = sample_generator(test_samples, batch_size = 32)\n",
    "    \n",
    "    model = model_setup(input_shape = (500, 500, 3), num_classes = len(LABEL_LIST))\n",
    "    history_object = model.fit_generator(train_generator, samples_per_epoch= int(len(train_samples)), validation_data=validation_generator, nb_val_samples=len(validation_samples), nb_epoch=1, verbose=1)\n",
    "    score = model.evaluate_generator(test_generator, 1500, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "    print(score)\n",
    "    \n",
    "    \n",
    "\n",
    "flow_setup()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
